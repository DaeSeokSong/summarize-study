{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DaeSeokSong/summarize-study/blob/master/%5BPractice%5D%20Computer%20Vision%20Model/RCNN/Faster_RCNN_training.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fpCBLtJpgn5q",
        "outputId": "39bff147-89a7-4c73-c401-a8701f44ba5a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cd /content/drive/MyDrive/Colab_Notebooks/FasterRCNN/Faster-RCNN-with-torchvision-master/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IPQinmo6gzUi",
        "outputId": "0e4b5924-7558-4a24-d110-74611d23bc0b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/Colab_Notebooks/FasterRCNN/Faster-RCNN-with-torchvision-master\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import utils\n",
        "import dataset.transforms as T\n",
        "import datetime\n",
        "import os\n",
        "import time\n",
        "\n",
        "import torch\n",
        "import torch.utils.data\n",
        "from torch import nn\n",
        "from dataset.coco_utils import get_coco, get_coco_kp\n",
        "from engine import train_one_epoch, evaluate\n",
        "from dataset.group_by_aspect_ratio import GroupedBatchSampler, create_aspect_ratio_groups\n",
        "import argparse\n",
        "import torchvision\n",
        "\n",
        "import cv2\n",
        "import random\n"
      ],
      "metadata": {
        "id": "LczAfF1Whhkm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "data_path = '../../data/coco/'\n",
        "model_name = 'fasterrcnn_resnet50_fpn'\n",
        "dataset_name = 'coco'\n",
        "device_type = 'cuda'\n",
        "batch_size = 8\n",
        "epochs = 20\n",
        "workers = 8\n",
        "lr = 0.02\n",
        "momentum = 0.9 \n",
        "weight_decay = 0.0001\n",
        "print_freq = 20\n",
        "lr_step_size = 8\n",
        "lr_steps = [8,11]\n",
        "lr_gamma = 0.1\n",
        "resume = ''\n",
        "test_only = True\n",
        "output_dir = './result'\n",
        "aspect_ratio_group_factor = 0\n",
        "pretrained = True\n",
        "distributed = False\n",
        "parallel = False\n",
        "world_size =1\n",
        "dist_url = 'env://'\n",
        "\n"
      ],
      "metadata": {
        "id": "GOy8uCsehNBD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_dataset(name, image_set, transform):\n",
        "    paths = {\n",
        "        \"coco\": ('../../data/coco/', get_coco, 91),\n",
        "        \"coco_kp\": ('/datasets01/COCO/022719/', get_coco_kp, 2)\n",
        "    }\n",
        "    p, ds_fn, num_classes = paths[name]\n",
        "\n",
        "    ds = ds_fn(p, image_set=image_set, transforms=transform)\n",
        "    return ds, num_classes"
      ],
      "metadata": {
        "id": "AyZV8yMDiQC_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def get_transform(train):\n",
        "    transforms = []\n",
        "    transforms.append(T.ToTensor())\n",
        "    if train:\n",
        "        transforms.append(T.RandomHorizontalFlip(0.5))\n",
        "    return T.Compose(transforms)"
      ],
      "metadata": {
        "id": "2hqel50WaHvn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "    if output_dir:\n",
        "        utils.mkdir(output_dir)    \n",
        "\n",
        "    # Data loading\n",
        "    print(\"Loading data\")\n",
        "    dataset, num_classes = get_dataset(dataset_name, \"train\", get_transform(train=True))\n",
        "    dataset_test, _ = get_dataset(dataset_name, \"val\", get_transform(train=False))    \n",
        "\n",
        "    print(\"Creating data loaders\")\n",
        "    if distributed:\n",
        "        train_sampler = torch.utils.data.distributed.DistributedSampler(dataset)\n",
        "        test_sampler = torch.utils.data.distributed.DistributedSampler(dataset_test)\n",
        "    else:\n",
        "        train_sampler = torch.utils.data.RandomSampler(dataset)\n",
        "        test_sampler = torch.utils.data.SequentialSampler(dataset_test)\n",
        "\n",
        "    if aspect_ratio_group_factor >= 0:\n",
        "        group_ids = create_aspect_ratio_groups(dataset, k=aspect_ratio_group_factor)\n",
        "        train_batch_sampler = GroupedBatchSampler(train_sampler, group_ids, batch_size)\n",
        "    else:\n",
        "        train_batch_sampler = torch.utils.data.BatchSampler(\n",
        "            train_sampler, batch_size, drop_last=True)\n",
        "\n",
        "    data_loader = torch.utils.data.DataLoader(\n",
        "        dataset, batch_sampler=train_batch_sampler, num_workers=workers,\n",
        "        collate_fn=utils.collate_fn)\n",
        "\n",
        "    data_loader_test = torch.utils.data.DataLoader(\n",
        "        dataset_test, batch_size=batch_size,\n",
        "        sampler=test_sampler, num_workers=workers,\n",
        "        collate_fn=utils.collate_fn)\n",
        "\n",
        "    # Model creating\n",
        "    print(\"Creating model\")\n",
        "    # model = models.__dict__[model](num_classes=num_classes, pretrained=pretrained)   \n",
        "    model = torchvision.models.detection.__dict__[model_name](num_classes=num_classes,\n",
        "                                                              pretrained=pretrained)\n",
        "\n",
        "    device = torch.device(device_type)\n",
        "    model.to(device)\n",
        "\n",
        "    # Distribute\n",
        "    model_without_ddp = model\n",
        "    if distributed:\n",
        "        model = torch.nn.parallel.DistributedDataParallel(model, device_ids=[gpu])\n",
        "        model_without_ddp = model.module    \n",
        "\n",
        "    # Parallel\n",
        "    if parallel:\n",
        "        print('Training parallel')\n",
        "        model = torch.nn.DataParallel(model).cuda()\n",
        "        model_without_ddp = model.module\n",
        "\n",
        "    # Optimizer\n",
        "    params = [p for p in model.parameters() if p.requires_grad]\n",
        "\n",
        "    optimizer = torch.optim.SGD(\n",
        "        params, lr=lr, momentum=momentum, weight_decay=weight_decay)\n",
        "\n",
        "    lr_scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=lr_steps, gamma=lr_gamma)\n",
        "\n",
        "    # Resume training\n",
        "    if resume:\n",
        "        print('Resume training')\n",
        "        checkpoint = torch.load(resume, map_location='cpu')\n",
        "        model_without_ddp.load_state_dict(checkpoint['model'])\n",
        "        optimizer.load_state_dict(checkpoint['optimizer'])\n",
        "        lr_scheduler.load_state_dict(checkpoint['lr_scheduler'])\n",
        "\n",
        "    if test_only:\n",
        "        evaluate(model, data_loader_test, device=device)\n",
        "        return\n",
        "\n",
        "    # Training\n",
        "    print('Start training')\n",
        "    start_time = time.time()\n",
        "    for epoch in range(epochs):\n",
        "        train_one_epoch(model, optimizer, data_loader, device, epoch, print_freq)\n",
        "        lr_scheduler.step()\n",
        "        if output_dir:\n",
        "            utils.save_on_master({\n",
        "                'model': model_without_ddp.state_dict(),\n",
        "                'optimizer': optimizer.state_dict(),\n",
        "                'lr_scheduler': lr_scheduler.state_dict()\n",
        "                },\n",
        "                os.path.join(output_dir, 'model_{}.pth'.format(epoch)))\n",
        "\n",
        "        # evaluate after every epoch\n",
        "        evaluate(model, data_loader_test, device=device)\n",
        "\n",
        "    total_time = time.time() - start_time\n",
        "    total_time_str = str(datetime.timedelta(seconds=int(total_time)))\n",
        "    print('Training time {}'.format(total_time_str))"
      ],
      "metadata": {
        "id": "GIOEzSXghRY2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zmk8i1kmhV_1",
        "outputId": "fb9e6813-c708-4f94-ed39-ef679cf938c8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading data\n",
            "loading annotations into memory...\n",
            "Done (t=18.48s)\n",
            "creating index...\n",
            "index created!\n",
            "loading annotations into memory...\n",
            "Done (t=0.52s)\n",
            "creating index...\n",
            "index created!\n",
            "Creating data loaders\n",
            "Using [0, 1.0, inf] as bins for aspect ratio quantization\n",
            "Count of instances per bin: [85308 31958]\n",
            "Creating model\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:566: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "/usr/local/lib/python3.7/dist-packages/torchvision/models/_utils.py:209: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and will be removed in 0.15, please use 'weights' instead.\n",
            "  f\"The parameter '{pretrained_param}' is deprecated since 0.13 and will be removed in 0.15, \"\n",
            "/usr/local/lib/python3.7/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=FasterRCNN_ResNet50_FPN_Weights.COCO_V1`. You can also use `weights=FasterRCNN_ResNet50_FPN_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test:  [  0/625]  eta: 0:50:24  model_time: 2.4013 (2.4013)  evaluator_time: 0.0688 (0.0688)  time: 4.8394  data: 2.2993  max mem: 4830\n",
            "Test:  [100/625]  eta: 0:06:58  model_time: 0.6577 (0.6733)  evaluator_time: 0.0543 (0.0516)  time: 0.7661  data: 0.0339  max mem: 5860\n",
            "Test:  [200/625]  eta: 0:05:30  model_time: 0.6592 (0.6639)  evaluator_time: 0.0506 (0.0523)  time: 0.7745  data: 0.0348  max mem: 5895\n",
            "Test:  [300/625]  eta: 0:04:11  model_time: 0.6599 (0.6584)  evaluator_time: 0.0483 (0.0597)  time: 0.7452  data: 0.0332  max mem: 5895\n",
            "Test:  [400/625]  eta: 0:02:53  model_time: 0.6604 (0.6569)  evaluator_time: 0.0414 (0.0574)  time: 0.7556  data: 0.0364  max mem: 5895\n",
            "Test:  [500/625]  eta: 0:01:35  model_time: 0.6327 (0.6546)  evaluator_time: 0.0407 (0.0565)  time: 0.7238  data: 0.0334  max mem: 5895\n",
            "Test:  [600/625]  eta: 0:00:19  model_time: 0.6592 (0.6529)  evaluator_time: 0.0551 (0.0569)  time: 0.7379  data: 0.0366  max mem: 5895\n",
            "Test:  [624/625]  eta: 0:00:00  model_time: 0.6602 (0.6529)  evaluator_time: 0.0512 (0.0566)  time: 0.7638  data: 0.0335  max mem: 5895\n",
            "Test: Total time: 0:07:57 (0.7633 s / it)\n",
            "Averaged stats: model_time: 0.6602 (0.6529)  evaluator_time: 0.0512 (0.0566)\n",
            "Accumulating evaluation results...\n",
            "DONE (t=5.97s).\n",
            "IoU metric: bbox\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.369\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.585\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.397\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.212\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.403\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.478\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.306\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.485\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.509\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.317\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.545\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.647\n"
          ]
        }
      ]
    }
  ]
}